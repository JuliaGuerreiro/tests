README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA2/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA2/lextest.cc
 mycoolc         -> [course dir]/src/PA2/mycoolc
 stringtab.cc    -> [course dir]/src/PA2/stringtab.cc
 utilities.cc    -> [course dir]/src/PA2/utilities.cc
 handle_flags.cc -> [course dir]/src/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. Information on how to do this
	is in the flex manual, which is part of your reader.

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% make lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% make dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	To turnin your work type:

	% make submit-clean

	After that,  collect the files cool.flex, test.cl,
	README, and test.output into a .tar.gz or .zip file and submit
	in our moodle page. Don't forget to edit the README file to
	include your write-up, and to write your own test cases in
	test.cl.

 	You may turn in the assignment as many times as you like.
	However, only the last version will be retained for
	grading.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------

## Documentation

### Design decisions

1. Lexical analysis: The scanner is responsible for performing lexical analysis on the input code, which involves breaking it down into individual tokens. Tokens represent the smallest meaningful units of code, such as keywords, operators, identifiers, and constants.

2. Token definitions: The scanner defines regular expressions to match different tokens in the input code. Each regular expression is associated with a corresponding action or token type. For example, the regular expression `(?i:CLASS)` matches the keyword "class" in a case-insensitive manner and returns the token type `CLASS`.

3. Handling comments: The scanner includes rules to handle single-line and block comments. When a comment is encountered, the scanner changes its state to ignore the characters until the comment ends.

4. String constants: The scanner handles string constants enclosed in double quotes (e.g., `"Hello, world!"`). It recognizes escape sequences such as `\"`, `\\`, `\n`, `\t`, etc., and assembles the string content while considering the maximum length and potential errors like unterminated strings or strings containing null characters.

5. Handling integers and identifiers: The scanner defines regular expressions to match integers and identifiers (type and object names). Integers are composed of one or more digits, while identifiers start with a letter (either lowercase or uppercase) and can include letters, digits, and underscores.

6. Error handling: The scanner includes a catch-all rule represented by the `.` regular expression to handle unrecognized characters or any other lexical errors. It captures the error message and returns an error token.

7. Additional definitions and utilities: The scanner includes additional definitions for utility functions, external dependencies, and global variables used by the scanner and other parts of the COOL compiler.

### Why our code is correct

1. Syntax compliance: The code follows the syntax rules of Flex, a widely used scanner generator. It defines lexical rules using regular expressions and associated actions. The code structure, including the use of `%{ %}` blocks, sections, and rules, aligns with the expected syntax of a Flex-based scanner definition.

2. Handling of special cases: The code handles special cases effectively. It includes rules to handle comments (both single-line and block comments) and string constants with escape sequences. It also addresses potential errors like unterminated string constants or strings containing null characters.

3. Token definitions: The code defines various token types based on regular expressions, including keywords, operators, integer constants, and identifiers (both type identifiers and object identifiers). These token definitions capture the lexical units of the COOL language, enabling subsequent stages of the compiler to process the code correctly.

4. Error handling: The code includes rules to capture and handle errors. When encountering unrecognized characters or other lexical errors, the code captures the error message and returns an error token, allowing the compiler to provide meaningful feedback to the user.

5. Integration with other COOL components: The code incorporates declarations and definitions required for integration with other components of the COOL compiler. It includes declarations of external functions and variables, such as `cool_yylval`, `yytext`, and `cool_yylval.error_msg`, which are expected to be defined and utilized in other parts of the compiler.

### Why our test cases are adequate

1. Coverage of token types: The test cases cover different token types, including numbers, identifiers, operators, and strings. By testing these specific token types, you ensure that the lexer can recognize and create the corresponding tokens correctly.

2. Validation of attribute values: Each token subclass has its own attributes (`value`, `id`, `operator`, and `str`). The test cases initialize these attributes and verify that they are set correctly. This helps ensure that the lexer properly assigns the attribute values to the corresponding tokens.

3. Verification of token names: The `name` attribute of each token subclass is set to a specific value ("NUMBER", "IDENTIFIER", "OPERATOR", "STRING") during initialization. The test cases validate that the `name` attribute is assigned the expected value, which is useful for verifying the correctness of token categorization.

4. Inheritance testing: The test cases demonstrate inheritance by using subclasses (`NumberToken`, `IdentifierToken`, etc.) that inherit from the base `Token` class. This allows you to ensure that the inheritance relationship is correctly implemented and that the attributes and behaviors of the base class are inherited by the subclasses.

5. Modular and self-contained: Each token subclass is implemented as a separate class with its own initialization method. This modularity and separation make the test cases more focused and easier to understand. It allows for targeted testing of individual token types without interfering with the functionality of other tokens.
